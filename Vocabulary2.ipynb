{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import codecs\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_words=[]\n",
    "MIN_WORD_FREQUENCY = 12\n",
    "vocabulary = \"vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(corpus_filename,isJoke,Tag=\"\"):\n",
    "    with open(corpus_filename) as myfile:\n",
    "        head = [next(myfile) for x in range(2000)]\n",
    "    size= len(head)\n",
    "    totalline =[\"\"]*size\n",
    "    for i in range(size):\n",
    "        if isJoke == True:\n",
    "            x = head[i].split(',',1)\n",
    "        else:\n",
    "            x = head[i].split('\\t')\n",
    "            \n",
    "        myline = x[1].replace(\".\", \" .\")\n",
    "        myline = x[1].replace(\"\\n\", \"  \\n\")\n",
    "        totalline[i]= Tag + myline\n",
    "    return totalline\n",
    "\n",
    "def get_text_in_words(tline):\n",
    "    get_this_text_in_words=[]\n",
    "    for line in tline:\n",
    "        word = line.split(' ')\n",
    "        for eachword in word:\n",
    "            if eachword !='':\n",
    "                get_this_text_in_words.append(eachword)\n",
    "    return get_this_text_in_words\n",
    "                \n",
    "def print_vocabulary(words_file_path, words_set):\n",
    "    words_file = codecs.open(words_file_path, 'w', encoding='utf8')\n",
    "    for w in words_set:\n",
    "        if w != \"\\n\":\n",
    "            words_file.write(w+\"\\n\")\n",
    "        else:\n",
    "            words_file.write(w)\n",
    "    words_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all dataset \n",
    "quotedataset= get_corpus(\"author-quote.txt\",False)\n",
    "jokedataset= get_corpus(\"shortjokes.csv\",True)\n",
    " \n",
    "#text_in_words = quotedataset + jokedataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill up the text_in_words with all dataset\n",
    "quote_text_in_words= get_text_in_words(quotedataset)\n",
    "joke_text_in_words= get_text_in_words(jokedataset)\n",
    "text_in_words = quote_text_in_words+ joke_text_in_words +quote_text_in_words+ joke_text_in_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the freq count \n",
    "sorted_x = sorted(word_freq.items(), key=operator.itemgetter(1))\n",
    "sorted_x.reverse()\n",
    "\n",
    "# to print out the freq only. \n",
    "#sorted_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 8000),\n",
       " ('the', 6168),\n",
       " ('a', 5150),\n",
       " ('to', 4470),\n",
       " ('I', 3824),\n",
       " ('and', 3396),\n",
       " ('of', 3358),\n",
       " ('is', 2590),\n",
       " ('in', 2434),\n",
       " ('you', 1964),\n",
       " ('that', 1600),\n",
       " ('my', 1274),\n",
       " ('have', 1216),\n",
       " ('for', 1128),\n",
       " ('it', 1124),\n",
       " ('with', 1016),\n",
       " ('do', 1016),\n",
       " ('be', 1016),\n",
       " ('are', 970),\n",
       " ('was', 930),\n",
       " ('on', 846),\n",
       " ('not', 840),\n",
       " ('The', 708),\n",
       " ('but', 690),\n",
       " ('like', 642),\n",
       " ('they', 634),\n",
       " ('as', 614),\n",
       " ('your', 602),\n",
       " ('we', 586),\n",
       " ('\"What', 564),\n",
       " ('can', 558),\n",
       " ('an', 556),\n",
       " (\"I'm\", 554),\n",
       " ('who', 552),\n",
       " ('at', 544),\n",
       " ('when', 532),\n",
       " ('me', 532),\n",
       " (\"don't\", 528),\n",
       " ('just', 528),\n",
       " ('people', 518),\n",
       " ('all', 506),\n",
       " ('or', 500),\n",
       " ('about', 488),\n",
       " ('so', 484),\n",
       " ('one', 480),\n",
       " ('what', 444),\n",
       " ('get', 432),\n",
       " ('\"I', 424),\n",
       " ('from', 404),\n",
       " ('think', 394),\n",
       " ('this', 390),\n",
       " ('more', 388),\n",
       " ('by', 384),\n",
       " ('has', 366),\n",
       " ('if', 364),\n",
       " ('than', 358),\n",
       " ('up', 356),\n",
       " ('out', 354),\n",
       " ('will', 354),\n",
       " ('his', 346),\n",
       " ('know', 342),\n",
       " ('A', 340),\n",
       " (\"it's\", 330),\n",
       " ('-', 328),\n",
       " ('because', 326),\n",
       " ('he', 326),\n",
       " ('It', 320),\n",
       " ('their', 314),\n",
       " ('into', 308),\n",
       " ('always', 308),\n",
       " ('no', 304),\n",
       " ('only', 302),\n",
       " ('make', 294),\n",
       " ('really', 294),\n",
       " ('want', 294),\n",
       " ('did', 292),\n",
       " ('very', 292),\n",
       " ('been', 288),\n",
       " ('You', 282),\n",
       " ('\"Why', 276),\n",
       " ('would', 272),\n",
       " ('call', 270),\n",
       " ('had', 268),\n",
       " ('which', 262),\n",
       " ('never', 262),\n",
       " ('say', 256),\n",
       " ('our', 250),\n",
       " (\"It's\", 240),\n",
       " ('other', 234),\n",
       " ('there', 232),\n",
       " ('We', 230),\n",
       " ('does', 224),\n",
       " ('love', 222),\n",
       " ('her', 212),\n",
       " ('were', 208),\n",
       " ('go', 208),\n",
       " ('see', 206),\n",
       " ('them', 204),\n",
       " ('many', 202),\n",
       " ('life', 202),\n",
       " ('good', 202),\n",
       " (\"I've\", 200),\n",
       " ('man', 198),\n",
       " ('take', 192),\n",
       " ('am', 192),\n",
       " ('got', 190),\n",
       " ('how', 190),\n",
       " ('it.', 188),\n",
       " ('any', 188),\n",
       " ('much', 188),\n",
       " ('going', 186),\n",
       " ('Because', 186),\n",
       " ('When', 186),\n",
       " ('He', 186),\n",
       " ('If', 186),\n",
       " ('two', 184),\n",
       " ('\"How', 182),\n",
       " ('\"My', 180),\n",
       " ('some', 180),\n",
       " ('But', 180),\n",
       " ('And', 178),\n",
       " ('could', 176),\n",
       " ('They', 174),\n",
       " ('between', 172),\n",
       " ('then', 168),\n",
       " ('In', 168),\n",
       " ('every', 168),\n",
       " ('time', 166),\n",
       " ('\"What\\'s', 164),\n",
       " ('world', 164),\n",
       " ('must', 162),\n",
       " (\"can't\", 162),\n",
       " ('things', 160),\n",
       " ('where', 160),\n",
       " ('should', 160),\n",
       " ('those', 160),\n",
       " ('she', 158),\n",
       " ('most', 158),\n",
       " ('first', 154),\n",
       " ('thing', 150),\n",
       " ('look', 150),\n",
       " ('work', 150),\n",
       " ('great', 148),\n",
       " ('There', 146),\n",
       " ('\"A', 144),\n",
       " ('lot', 144),\n",
       " ('feel', 142),\n",
       " ('too', 142),\n",
       " ('something', 140),\n",
       " ('being', 138),\n",
       " ('way', 136),\n",
       " (\"you're\", 136),\n",
       " ('own', 134),\n",
       " ('put', 132),\n",
       " ('little', 132),\n",
       " ('even', 128),\n",
       " ('My', 128),\n",
       " ('over', 126),\n",
       " (\"that's\", 126),\n",
       " ('new', 124),\n",
       " ('said', 124),\n",
       " ('us', 124),\n",
       " ('through', 122),\n",
       " ('after', 122),\n",
       " ('still', 120),\n",
       " ('difference', 118),\n",
       " ('without', 118),\n",
       " ('believe', 116),\n",
       " ('come', 114),\n",
       " ('ever', 114),\n",
       " ('part', 112),\n",
       " ('men', 112),\n",
       " ('him', 112),\n",
       " ('back', 110),\n",
       " ('best', 110),\n",
       " ('What', 110),\n",
       " ('tell', 108),\n",
       " ('kind', 106),\n",
       " (\"didn't\", 104),\n",
       " ('live', 104),\n",
       " ('\"If', 102),\n",
       " ('hear', 102),\n",
       " ('down', 102),\n",
       " ('its', 102),\n",
       " ('made', 102),\n",
       " ('need', 100),\n",
       " ('Me:', 98),\n",
       " ('says', 98),\n",
       " (\"doesn't\", 98),\n",
       " ('same', 96),\n",
       " ('someone', 94),\n",
       " ('person', 94),\n",
       " ('give', 94),\n",
       " ('around', 94),\n",
       " ('right', 92),\n",
       " ('me.', 90),\n",
       " ('One', 90),\n",
       " ('To', 90),\n",
       " ('\"The', 88),\n",
       " ('find', 88),\n",
       " ('walks', 86),\n",
       " ('wife', 86),\n",
       " ('why', 86),\n",
       " ('also', 86),\n",
       " ('No', 86),\n",
       " ('music', 84),\n",
       " ('used', 84),\n",
       " ('having', 84),\n",
       " ('old', 84),\n",
       " ('myself', 84),\n",
       " ('human', 84),\n",
       " ('before', 84),\n",
       " ('guy', 82),\n",
       " ('off', 82),\n",
       " ('play', 82),\n",
       " ('important', 82),\n",
       " ('So', 82),\n",
       " (\"I'd\", 82),\n",
       " ('while', 80),\n",
       " ('change', 80),\n",
       " ('makes', 80),\n",
       " ('women', 80),\n",
       " ('often', 80),\n",
       " ('better', 80),\n",
       " ('me,', 80),\n",
       " ('day', 80),\n",
       " ('told', 78),\n",
       " ('wanted', 78),\n",
       " ('comes', 78),\n",
       " ('called', 78),\n",
       " ('thought', 78),\n",
       " ('both', 76),\n",
       " ('hard', 76),\n",
       " ('long', 76),\n",
       " ('anything', 76),\n",
       " ('may', 76),\n",
       " ('making', 76),\n",
       " ('them.', 76),\n",
       " ('bad', 74),\n",
       " ('black', 74),\n",
       " ('big', 74),\n",
       " ('pretty', 74),\n",
       " ('woman', 72),\n",
       " ('last', 72),\n",
       " ('try', 72),\n",
       " (\"That's\", 70),\n",
       " ('different', 70),\n",
       " ('now', 70),\n",
       " ('it,', 70),\n",
       " ('girl', 68),\n",
       " ('years', 68),\n",
       " ('sure', 68),\n",
       " ('getting', 68),\n",
       " ('life.', 68),\n",
       " ('happy', 68),\n",
       " ('She', 66),\n",
       " (\"they're\", 66),\n",
       " ('God', 66),\n",
       " ('these', 66),\n",
       " ('next', 64),\n",
       " ('such', 64),\n",
       " ('let', 64),\n",
       " ('went', 64),\n",
       " ('doing', 64),\n",
       " ('This', 64),\n",
       " ('joke', 62),\n",
       " ('light', 62),\n",
       " ('real', 62),\n",
       " ('cannot', 62),\n",
       " ('everything', 62),\n",
       " ('looking', 62),\n",
       " ('place', 62),\n",
       " ('job', 62),\n",
       " ('gets', 60),\n",
       " ('For', 60),\n",
       " ('home', 60),\n",
       " ('use', 60),\n",
       " ('each', 60),\n",
       " (\"I'll\", 60),\n",
       " ('working', 60),\n",
       " ('That', 60),\n",
       " ('that.', 60),\n",
       " ('actually', 58),\n",
       " ('life,', 58),\n",
       " ('came', 58),\n",
       " ('public', 58),\n",
       " ('become', 58),\n",
       " ('whole', 58),\n",
       " ('well', 58),\n",
       " ('?', 56),\n",
       " ('...', 56),\n",
       " ('\"Did', 56),\n",
       " ('found', 56),\n",
       " ('hate', 56),\n",
       " ('high', 56),\n",
       " ('asked', 56),\n",
       " (\"there's\", 56),\n",
       " ('keep', 56),\n",
       " ('you.', 56),\n",
       " ('i', 54),\n",
       " ('help', 54),\n",
       " ('few', 54),\n",
       " ('understand', 54),\n",
       " ('family', 54),\n",
       " ('film', 54),\n",
       " ('done', 54),\n",
       " ('nothing', 54),\n",
       " ('saw', 54),\n",
       " ('friend', 54),\n",
       " ('three', 54),\n",
       " ('favorite', 52),\n",
       " ('walk', 52),\n",
       " ('less', 52),\n",
       " ('mind', 52),\n",
       " ('once', 52),\n",
       " ('.', 50),\n",
       " ('it.\"', 50),\n",
       " ('able', 50),\n",
       " ('American', 50),\n",
       " ('New', 50),\n",
       " ('All', 50),\n",
       " ('today', 50),\n",
       " ('\"I\\'m', 48),\n",
       " ('dog', 48),\n",
       " ('end', 48),\n",
       " (\"couldn't\", 48),\n",
       " ('sense', 48),\n",
       " ('reason', 48),\n",
       " ('show', 48),\n",
       " ('might', 48),\n",
       " ('leave', 48),\n",
       " ('read', 48),\n",
       " ('gay', 46),\n",
       " ('feeling', 46),\n",
       " ('baby', 46),\n",
       " (\"Don't\", 46),\n",
       " (\"wasn't\", 46),\n",
       " ('mean', 46),\n",
       " ('means', 46),\n",
       " (\"we're\", 46),\n",
       " ('time,', 46),\n",
       " ('idea', 46),\n",
       " ('eat', 46),\n",
       " ('others', 46),\n",
       " ('\"When', 44),\n",
       " ('year', 44),\n",
       " ('pay', 44),\n",
       " ('until', 44),\n",
       " ('&', 44),\n",
       " ('friends', 44),\n",
       " ('goes', 44),\n",
       " ('Every', 44),\n",
       " ('People', 44),\n",
       " ('As', 44),\n",
       " ('everyone', 44),\n",
       " ('true', 44),\n",
       " ('people.', 44),\n",
       " ('times', 44),\n",
       " ('greatest', 44),\n",
       " ('wrong', 44),\n",
       " ('start', 44),\n",
       " ('girlfriend', 42),\n",
       " ('sex', 42),\n",
       " (\"he's\", 42),\n",
       " ('yourself', 42),\n",
       " ('learn', 42),\n",
       " ('left', 42),\n",
       " ('hold', 42),\n",
       " ('Why', 42),\n",
       " ('At', 42),\n",
       " ('people,', 42),\n",
       " ('school', 42),\n",
       " ('do.', 42),\n",
       " ('run', 42),\n",
       " ('bring', 42),\n",
       " ('Most', 42),\n",
       " ('country', 42),\n",
       " ('anyone', 42),\n",
       " ('cross', 40),\n",
       " ('How', 40),\n",
       " ('buy', 40),\n",
       " ('saying', 40),\n",
       " (\"isn't\", 40),\n",
       " ('another', 40),\n",
       " ('gonna', 40),\n",
       " ('gave', 40),\n",
       " ('guess', 40),\n",
       " (\"won't\", 40),\n",
       " ('young', 40),\n",
       " ('body', 40),\n",
       " ('whether', 40),\n",
       " ('\"You', 38),\n",
       " ('mom', 38),\n",
       " ('hand', 38),\n",
       " (\"They're\", 38),\n",
       " ('under', 38),\n",
       " ('you,', 38),\n",
       " ('far', 38),\n",
       " ('rather', 38),\n",
       " ('heart', 38),\n",
       " (\"wouldn't\", 38),\n",
       " ('living', 38),\n",
       " ('is,', 38),\n",
       " ('here', 38),\n",
       " ('certain', 38),\n",
       " ('problem', 38),\n",
       " ('win', 38),\n",
       " ('half', 38),\n",
       " (\"There's\", 38),\n",
       " ('lost', 38),\n",
       " ('money', 38),\n",
       " ('stop', 38),\n",
       " ('!\"', 36),\n",
       " ('\"\"I', 36),\n",
       " ('cut', 36),\n",
       " ('write', 36),\n",
       " ('least', 36),\n",
       " ('trying', 36),\n",
       " ('probably', 36),\n",
       " ('world.', 36),\n",
       " ('number', 36),\n",
       " ('matter', 36),\n",
       " ('care', 36),\n",
       " ('enough', 36),\n",
       " ('guys', 36),\n",
       " ('days', 36),\n",
       " ('took', 36),\n",
       " ('hope', 36),\n",
       " ('dance', 36),\n",
       " ('talk', 36),\n",
       " ('kids', 36),\n",
       " ('full', 36),\n",
       " ('enjoy', 36),\n",
       " ('power', 36),\n",
       " ('learned', 36),\n",
       " ('loved', 36),\n",
       " ('wants', 36),\n",
       " ('watch', 36),\n",
       " ('boy', 34),\n",
       " ('asks', 34),\n",
       " ('3', 34),\n",
       " ('ME:', 34),\n",
       " ('dead', 34),\n",
       " (\"you've\", 34),\n",
       " ('An', 34),\n",
       " ('wear', 34),\n",
       " ('using', 34),\n",
       " ('control', 34),\n",
       " ('truth', 34),\n",
       " ('spend', 34),\n",
       " ('fight', 34),\n",
       " ('word', 34),\n",
       " ('second', 34),\n",
       " ('face', 34),\n",
       " ('deal', 34),\n",
       " ('quite', 34),\n",
       " ('way.', 34),\n",
       " ('bit', 34),\n",
       " ('talking', 34),\n",
       " ('children', 34),\n",
       " ('behind', 34),\n",
       " ('name', 34),\n",
       " ('sort', 34),\n",
       " ('\"Two', 32),\n",
       " ('already', 32),\n",
       " ('realize', 32),\n",
       " ('Now', 32),\n",
       " ('started', 32),\n",
       " ('girls', 32),\n",
       " ('say,', 32),\n",
       " ('worse', 32),\n",
       " ('house', 32),\n",
       " ('room', 32),\n",
       " ('lose', 32),\n",
       " ('played', 32),\n",
       " ('felt', 32),\n",
       " ('Do', 32),\n",
       " ('Not', 32),\n",
       " ('movie', 32),\n",
       " ('away', 32),\n",
       " ('Sometimes', 32),\n",
       " ('five', 32),\n",
       " ('know,', 32),\n",
       " ('remember', 32),\n",
       " ('free', 32),\n",
       " ('tried', 32),\n",
       " ('game', 32),\n",
       " ('simply', 32),\n",
       " ('\"Just', 30),\n",
       " ('art', 30),\n",
       " ('marriage', 30),\n",
       " ('car', 30),\n",
       " ('heard', 30),\n",
       " ('create', 30),\n",
       " ('small', 30),\n",
       " ('said,', 30),\n",
       " (\"she's\", 30),\n",
       " ('completely', 30),\n",
       " ('together', 30),\n",
       " ('easy', 30),\n",
       " ('else', 30),\n",
       " ('blind', 30),\n",
       " ('easier', 30),\n",
       " ('greater', 30),\n",
       " ('doctor', 30),\n",
       " ('Just', 30),\n",
       " ('upon', 30),\n",
       " ('past', 30),\n",
       " ('kill', 30),\n",
       " ('now.', 30),\n",
       " ('almost', 30),\n",
       " ('certainly', 30),\n",
       " ('war', 30),\n",
       " ('America', 30),\n",
       " ('fun', 30),\n",
       " ('out.', 30),\n",
       " ('acting', 30),\n",
       " ('ask', 30),\n",
       " ('like,', 30),\n",
       " ('sitting', 30),\n",
       " ('knew', 30),\n",
       " ('eating', 28),\n",
       " ('white', 28),\n",
       " ('Turkey', 28),\n",
       " ('hit', 28),\n",
       " ('necessarily', 28),\n",
       " ('sometimes', 28),\n",
       " ('themselves', 28),\n",
       " ('die', 28),\n",
       " ('hair', 28),\n",
       " ('natural', 28),\n",
       " ('story', 28),\n",
       " ('ones', 28),\n",
       " ('since', 28),\n",
       " ('shall', 28),\n",
       " ('government', 28),\n",
       " ('please', 28),\n",
       " ('stand', 28),\n",
       " ('bar', 28),\n",
       " ('love,', 28),\n",
       " ('happens', 28),\n",
       " (\"what's\", 28),\n",
       " ('day.', 28),\n",
       " ('playing', 28),\n",
       " ('walking', 28),\n",
       " ('person.', 28),\n",
       " ('us.', 28),\n",
       " ('whatever', 28),\n",
       " ('world,', 28),\n",
       " ('forget', 28),\n",
       " ('2', 26),\n",
       " ('drink', 26),\n",
       " ('A:', 26),\n",
       " ('common?', 26),\n",
       " ('jokes', 26),\n",
       " ('group', 26),\n",
       " ('night', 26),\n",
       " ('knock', 26),\n",
       " ('looks', 26),\n",
       " ('wonder', 26),\n",
       " (\"He's\", 26),\n",
       " ('favourite', 26),\n",
       " ('listen', 26),\n",
       " ('music.', 26),\n",
       " ('open', 26),\n",
       " ('fall', 26),\n",
       " ('seen', 26),\n",
       " ('meet', 26),\n",
       " ('fell', 26),\n",
       " ('phone', 26),\n",
       " ('act', 26),\n",
       " ('stuff', 26),\n",
       " ('save', 26),\n",
       " ('turn', 26),\n",
       " ('four', 26),\n",
       " (\"one's\", 26),\n",
       " ('form', 26),\n",
       " ('faith', 26),\n",
       " ('worst', 26),\n",
       " ('definitely', 26),\n",
       " ('early', 26),\n",
       " ('piece', 26),\n",
       " (\"haven't\", 26),\n",
       " ('quality', 26),\n",
       " ('becomes', 26),\n",
       " ('good,', 26),\n",
       " ('history', 26),\n",
       " ('interested', 26),\n",
       " ('actor', 26),\n",
       " ('nobody', 26),\n",
       " ('seems', 26),\n",
       " ('thing.', 26),\n",
       " ('hands', 26),\n",
       " ('build', 26),\n",
       " ('experience', 26),\n",
       " ('problems', 26),\n",
       " ('dream', 26),\n",
       " ('do,', 26),\n",
       " ('her.', 26),\n",
       " ('respect', 26),\n",
       " ('mean,', 26),\n",
       " ('thinking', 26),\n",
       " ('stay', 26),\n",
       " (\"you'll\", 26),\n",
       " ('bulb?', 24),\n",
       " (\"What's\", 24),\n",
       " ('Trump', 24),\n",
       " ('dad', 24),\n",
       " ('type', 24),\n",
       " ('head', 24),\n",
       " ('worth', 24),\n",
       " ('terrible', 24),\n",
       " ('possible', 24),\n",
       " ('point', 24),\n",
       " ('front', 24),\n",
       " ('TV', 24),\n",
       " ('From', 24),\n",
       " ('kid', 24),\n",
       " ('during', 24),\n",
       " ('lead', 24),\n",
       " ('sleep', 24),\n",
       " ('Is', 24),\n",
       " ('death', 24),\n",
       " ('them,', 24),\n",
       " ('learning', 24),\n",
       " ('Michael', 24),\n",
       " ('nature', 24),\n",
       " ('given', 24),\n",
       " ('political', 24),\n",
       " ('Some', 24),\n",
       " ('gives', 24),\n",
       " ('drive', 24),\n",
       " ('words', 24),\n",
       " ('happiness', 24),\n",
       " ('Man', 24),\n",
       " ('God,', 24),\n",
       " ('work.', 24),\n",
       " ('move', 24),\n",
       " ('one.', 24),\n",
       " ('business', 24),\n",
       " ('up.', 24),\n",
       " ('says,', 22),\n",
       " ('police', 22),\n",
       " ('me.\"', 22),\n",
       " ('\"Me:', 22),\n",
       " ('\"Q:', 22),\n",
       " ('road?', 22),\n",
       " ('son', 22),\n",
       " ('Mexican', 22),\n",
       " ('door', 22),\n",
       " ('fat', 22),\n",
       " ('5', 22),\n",
       " ('Your', 22),\n",
       " ('himself', 22),\n",
       " ('8', 22),\n",
       " ('Those', 22),\n",
       " ('child', 22),\n",
       " ('red', 22),\n",
       " ('watching', 22),\n",
       " ('tells', 22),\n",
       " ('huge', 22),\n",
       " ('finally', 22),\n",
       " ('ways', 22),\n",
       " ('Americans', 22),\n",
       " ('works', 22),\n",
       " ('Our', 22),\n",
       " ('eyes', 22),\n",
       " ('career', 22),\n",
       " ('Everyone', 22),\n",
       " ('personal', 22),\n",
       " ('him,', 22),\n",
       " ('grew', 22),\n",
       " ('yourself.', 22),\n",
       " ('taken', 22),\n",
       " ('is.', 22),\n",
       " ('set', 22),\n",
       " ('United', 22),\n",
       " ('keeps', 22),\n",
       " ('there.', 22),\n",
       " ('coming', 22),\n",
       " ('time.', 22),\n",
       " ('Unless', 22),\n",
       " ('needs', 22),\n",
       " ('difficult', 22),\n",
       " ('all.', 22),\n",
       " ('mother', 22),\n",
       " ('beautiful', 22),\n",
       " ('takes', 22),\n",
       " ('future', 22),\n",
       " ('state', 22),\n",
       " ('that,', 22),\n",
       " (\"You're\", 22),\n",
       " ('u', 20),\n",
       " ('screw', 20),\n",
       " ('fucking', 20),\n",
       " ('bar...', 20),\n",
       " ('\"There', 20),\n",
       " ('\"This', 20),\n",
       " ('cow', 20),\n",
       " ('shit', 20),\n",
       " ('walked', 20),\n",
       " ('cat', 20),\n",
       " ('turns', 20),\n",
       " ('water', 20),\n",
       " ('leaving', 20),\n",
       " ('named', 20),\n",
       " ('decided', 20),\n",
       " ('dies', 20),\n",
       " (\"you'd\", 20),\n",
       " ('players', 20),\n",
       " ('funny', 20),\n",
       " ('age', 20),\n",
       " ('nice', 20),\n",
       " ('running', 20),\n",
       " ('trust', 20),\n",
       " ('everybody', 20),\n",
       " ('caught', 20),\n",
       " ('bought', 20),\n",
       " ('War', 20),\n",
       " ('others.', 20),\n",
       " ('writing', 20),\n",
       " ('sit', 20),\n",
       " ('know.', 20),\n",
       " ('though', 20),\n",
       " ('sick', 20),\n",
       " ('Christmas', 20),\n",
       " ('date', 20),\n",
       " ('Like', 20),\n",
       " ('step', 20),\n",
       " ('months', 20),\n",
       " ('smaller', 20),\n",
       " ('written', 20),\n",
       " ('along', 20),\n",
       " ('against', 20),\n",
       " ('large', 20),\n",
       " ('hours', 20),\n",
       " ('years.', 20),\n",
       " ('laugh', 20),\n",
       " ('grow', 20),\n",
       " ('forward', 20),\n",
       " ('anything.', 20),\n",
       " ('films', 20),\n",
       " ('taking', 20),\n",
       " ('seem', 20),\n",
       " ('Many', 20),\n",
       " ('fan', 20),\n",
       " ('born', 20),\n",
       " ('answer', 20),\n",
       " ('biggest', 20),\n",
       " ('knowledge', 20),\n",
       " ('national', 20),\n",
       " ('be.', 20),\n",
       " ('movies', 20),\n",
       " ('needed', 20),\n",
       " ('fans', 20),\n",
       " ('involved', 20),\n",
       " ('album', 20),\n",
       " ('liked', 20),\n",
       " ('sad', 20),\n",
       " ('chance', 20),\n",
       " ('Life', 20),\n",
       " ('students', 20),\n",
       " ('education', 20),\n",
       " ('success', 20),\n",
       " ('helps', 20),\n",
       " ('within', 20),\n",
       " ('Great', 20),\n",
       " ('figure', 20),\n",
       " ('much.', 20),\n",
       " ('key', 20),\n",
       " ('instead', 20),\n",
       " ('you.\"', 18),\n",
       " ('bed', 18),\n",
       " ('\"Don\\'t', 18),\n",
       " ('hole', 18),\n",
       " ('ate', 18),\n",
       " ('\"Who', 18),\n",
       " ('them.\"', 18),\n",
       " ('THE', 18),\n",
       " ('chicken', 18),\n",
       " ('blonde', 18),\n",
       " ('daughter', 18),\n",
       " ('legs', 18),\n",
       " ('picture', 18),\n",
       " ('taste', 18),\n",
       " ('short', 18),\n",
       " ('30', 18),\n",
       " ('lived', 18),\n",
       " ('stuck', 18),\n",
       " ('serve', 18),\n",
       " ('won', 18),\n",
       " ('late', 18),\n",
       " ('classical', 18),\n",
       " ('itself', 18),\n",
       " ('country.', 18),\n",
       " (\"who's\", 18),\n",
       " ('dress', 18),\n",
       " ('houses', 18),\n",
       " ('feed', 18),\n",
       " ('growing', 18),\n",
       " ('present', 18),\n",
       " ('joy', 18),\n",
       " ('died', 18),\n",
       " ('wait', 18),\n",
       " ('cause', 18),\n",
       " (\"We're\", 18),\n",
       " ('Even', 18),\n",
       " ('supposed', 18),\n",
       " ('built', 18),\n",
       " ('on.', 18),\n",
       " ('entire', 18),\n",
       " ('food', 18),\n",
       " ('radio', 18),\n",
       " ('Then', 18),\n",
       " ('bus', 18),\n",
       " ('speed', 18),\n",
       " ('Nothing', 18),\n",
       " ('sounds', 18),\n",
       " ('powerful', 18),\n",
       " ('turned', 18),\n",
       " ('book', 18),\n",
       " ('fool', 18),\n",
       " ('six', 18),\n",
       " (\"man's\", 18),\n",
       " ('wearing', 18),\n",
       " ('but,', 18),\n",
       " ('attention', 18),\n",
       " ('proud', 18),\n",
       " ('character', 18),\n",
       " ('fact', 18),\n",
       " ('him.', 18),\n",
       " ('today.', 18),\n",
       " ('ideas', 18),\n",
       " ('known', 18),\n",
       " ('emotional', 18),\n",
       " ('moment', 18),\n",
       " ('soul', 18),\n",
       " ('father', 18),\n",
       " ('secret', 18),\n",
       " (\"people's\", 18),\n",
       " ('things.', 18),\n",
       " ('Well,', 18),\n",
       " ('rich', 18),\n",
       " ('English', 18),\n",
       " ('community', 18),\n",
       " ('His', 18),\n",
       " ('peace', 18),\n",
       " ('support', 18),\n",
       " ('cool', 18),\n",
       " ('stage', 18),\n",
       " (\"God's\", 18),\n",
       " ('myself.', 18),\n",
       " ('sing', 18),\n",
       " ('comfortable', 18),\n",
       " ('Only', 18),\n",
       " ('success.', 18),\n",
       " ('mostly', 18),\n",
       " ('hundred', 18),\n",
       " ('\"\"You', 16),\n",
       " ('hot', 16),\n",
       " ('6', 16),\n",
       " ('dude', 16),\n",
       " ('\"They', 16),\n",
       " ('Jesus', 16),\n",
       " ('\"So', 16),\n",
       " ('replies', 16),\n",
       " ('12', 16),\n",
       " ('fuck', 16),\n",
       " ('cheese', 16),\n",
       " ('online', 16),\n",
       " ('1', 16),\n",
       " ('alone', 16),\n",
       " ('lie', 16),\n",
       " ('notice', 16),\n",
       " ('longer', 16),\n",
       " ('invented', 16),\n",
       " ('successful', 16),\n",
       " ('break', 16),\n",
       " ('Chinese', 16),\n",
       " ('unless', 16),\n",
       " ('question', 16),\n",
       " ('parents', 16),\n",
       " ('teaching', 16),\n",
       " ('harmony', 16),\n",
       " ('decide', 16),\n",
       " ('relationship', 16),\n",
       " ('accept', 16),\n",
       " ('usually', 16),\n",
       " ('inside', 16),\n",
       " ('better.', 16),\n",
       " ('with.', 16),\n",
       " ('reality', 16),\n",
       " ('sound', 16),\n",
       " ('fish', 16),\n",
       " ('single', 16),\n",
       " ('dogs', 16),\n",
       " ('pictures', 16),\n",
       " ('strong', 16),\n",
       " ('older', 16),\n",
       " ('Good', 16),\n",
       " ('home,', 16),\n",
       " ('together.', 16),\n",
       " ('culture', 16),\n",
       " ('common', 16),\n",
       " ('sign', 16),\n",
       " ('blood', 16),\n",
       " ('company', 16),\n",
       " ('day,', 16),\n",
       " ('close', 16),\n",
       " ('After', 16),\n",
       " ('absolutely', 16),\n",
       " ('On', 16),\n",
       " ('up,', 16),\n",
       " ('to.', 16),\n",
       " ('music,', 16),\n",
       " ('voice', 16),\n",
       " ('course,', 16),\n",
       " ('soon', 16),\n",
       " ('10', 16),\n",
       " ('drunk', 16),\n",
       " ('too.', 16),\n",
       " ('animal', 16),\n",
       " ('plenty', 16),\n",
       " ('economy', 16),\n",
       " ('rule', 16),\n",
       " ('simple', 16),\n",
       " ('Who', 16),\n",
       " ('Muslim', 16),\n",
       " ('morning', 16),\n",
       " ('themselves.', 16),\n",
       " (\"other's\", 16),\n",
       " ('tree', 16),\n",
       " ('other.', 16),\n",
       " ('fire', 16),\n",
       " ('all,', 16),\n",
       " ('fine', 16),\n",
       " ('towards', 16),\n",
       " ('view', 16),\n",
       " ('wish', 16),\n",
       " ('not,', 16),\n",
       " ('believed', 16),\n",
       " ('attitude', 16),\n",
       " ('values', 16),\n",
       " ('Day', 16),\n",
       " ('media', 16),\n",
       " ('spiritual', 16),\n",
       " ('spent', 16),\n",
       " ('creative', 16),\n",
       " ('anybody', 16),\n",
       " ('religious', 16),\n",
       " ('fighting', 16),\n",
       " ('owe', 16),\n",
       " ('nor', 16),\n",
       " ('again', 16),\n",
       " ('vote', 16),\n",
       " ('pass', 16),\n",
       " ('giving', 16),\n",
       " ('God.', 16),\n",
       " ('York', 16),\n",
       " ('private', 16),\n",
       " ('character.', 16),\n",
       " ('aware', 16),\n",
       " ('hard.', 16),\n",
       " ('fear', 16),\n",
       " ('role', 16),\n",
       " ('lives', 16),\n",
       " ('process', 16),\n",
       " ('strength', 16),\n",
       " ('failure', 16),\n",
       " ('courage', 16),\n",
       " ('about.', 16),\n",
       " ('financial', 16),\n",
       " ('popular', 16),\n",
       " ('Never', 16),\n",
       " ('Dog:', 14),\n",
       " ('hell', 14),\n",
       " ('pizza', 14),\n",
       " ('Two', 14),\n",
       " ('\"Do', 14),\n",
       " ('joke.', 14),\n",
       " ('up.\"', 14),\n",
       " ('YOU', 14),\n",
       " ('\"Sometimes', 14),\n",
       " ('midget', 14),\n",
       " ('bartender', 14),\n",
       " ('4', 14),\n",
       " ('\"\"Do', 14),\n",
       " ('\"Where', 14),\n",
       " ('it\"', 14),\n",
       " ('elephants', 14),\n",
       " ('forgot', 14),\n",
       " ('\"I\\'ve', 14),\n",
       " ('met', 14),\n",
       " ('third', 14),\n",
       " ('dark', 14),\n",
       " ('manager', 14),\n",
       " ('remind', 14),\n",
       " ('fake', 14),\n",
       " ('England', 14),\n",
       " ('click', 14),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 17285\n",
      "Ignoring words with frequency < 12\n",
      "Unique words after ignoring: 17285\n"
     ]
    }
   ],
   "source": [
    "words = set(text_in_words)\n",
    "words.add(\"#@Joke\")\n",
    "words.add(\"#@Quote\")\n",
    "words.add(\"#@NoTag\")\n",
    "\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "#words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "print_vocabulary(vocabulary, words)\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN =3\n",
    "MY_SEQUENCE_LEN =2\n",
    "STEP =1\n",
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "\n",
    "\n",
    "def processXandY(myText_in_words,tag=\"\"):\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "    for i in range(0, len(myText_in_words) - MY_SEQUENCE_LEN, STEP):\n",
    "        # Only add the sequences where no word is in ignored_words\n",
    "        if len(set(text_in_words[i: i+MY_SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "            mysentences = []\n",
    "            mysentences.append(tag)\n",
    "            for myword in myText_in_words[i: i + MY_SEQUENCE_LEN]:\n",
    "                mysentences.append(myword)\n",
    "            #sentences.append(myText_in_words[i: i + SEQUENCE_LEN])\n",
    "            sentences.append(mysentences)\n",
    "            next_words.append(myText_in_words[i + MY_SEQUENCE_LEN])\n",
    "        else:\n",
    "            ignored = ignored + 1\n",
    "    print('Ignored sequences:', ignored)\n",
    "    print('Remaining sequences:', len(sentences))\n",
    "    return sentences,next_words\n",
    "\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 20485\n",
      "Remaining sequences: 16399\n",
      "Ignored sequences: 20485\n",
      "Remaining sequences: 16399\n",
      "Ignored sequences: 27236\n",
      "Remaining sequences: 22329\n",
      "Ignored sequences: 27236\n",
      "Remaining sequences: 22329\n"
     ]
    }
   ],
   "source": [
    "Joke_Tag_Sentences,Joke_Tag_Next_Words =processXandY(joke_text_in_words,\"#@Joke\")\n",
    "Joke_NoTag_Sentences,Joke_NoTag_Next_Words =processXandY(joke_text_in_words,\"#@NoTag\")\n",
    "quote_Tag_Sentences,quote_Tag_Next_Words =processXandY(quote_text_in_words,\"#@Quote\")\n",
    "quote_NoTag_Sentences,quote_NoTag_Next_Words =processXandY(quote_text_in_words,\"#@NoTag\")\n",
    "\n",
    "sentences = Joke_Tag_Sentences + Joke_NoTag_Sentences + quote_Tag_Sentences + quote_NoTag_Sentences\n",
    "next_words = Joke_Tag_Next_Words + Joke_NoTag_Next_Words + quote_Tag_Next_Words + quote_NoTag_Next_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quote_NoTag_Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================end of preprocesing ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"examples2.txt\"\n",
    "BATCH_SIZE =64\n",
    "if not os.path.isdir('./checkpoints2/'):\n",
    "    os.makedirs('./checkpoints2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=0.1):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, 3, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 77222\n",
      "Size of test set = 78\n"
     ]
    }
   ],
   "source": [
    "  # x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
    "    sentences, next_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./checkpoints2/LSTM_try-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "                \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "                (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1207/1207 [==============================] - 153s 127ms/step - loss: 6.6210 - acc: 0.0468 - val_loss: 6.3727 - val_acc: 0.0703\n",
      "Epoch 2/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 5.6338 - acc: 0.1242 - val_loss: 5.5219 - val_acc: 0.1719\n",
      "Epoch 3/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 4.8460 - acc: 0.1936 - val_loss: 5.1566 - val_acc: 0.1562\n",
      "Epoch 4/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 4.1333 - acc: 0.2560 - val_loss: 4.6489 - val_acc: 0.1875\n",
      "Epoch 5/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 3.4758 - acc: 0.3335 - val_loss: 4.2602 - val_acc: 0.2656\n",
      "Epoch 6/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 2.9473 - acc: 0.3977 - val_loss: 3.9841 - val_acc: 0.2578\n",
      "Epoch 7/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 2.5709 - acc: 0.4415 - val_loss: 3.4834 - val_acc: 0.3281\n",
      "Epoch 8/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 2.3078 - acc: 0.4732 - val_loss: 3.1897 - val_acc: 0.3672\n",
      "Epoch 9/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 2.1255 - acc: 0.4930 - val_loss: 3.0785 - val_acc: 0.3594\n",
      "Epoch 10/100\n",
      "1207/1207 [==============================] - 151s 125ms/step - loss: 1.9895 - acc: 0.5099 - val_loss: 2.8517 - val_acc: 0.3750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8ebb8ac1cb4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a5f335019fad>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(epoch, logs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "examples_file = open(examples, \"w\")\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nex(sentence,diversity):\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x_pred[0, t, word_indices[word]] = 1.\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "    return next_word\n",
    " \n",
    "#sentence = [\"#@Joke\", \"I\",\"am\"]\n",
    "#predicted =predict_nex(sentence)\n",
    "\n",
    "def printSentence(sentence):\n",
    "    fullSentence =\"\"\n",
    "    for word in sentence:\n",
    "        fullSentence =fullSentence +\" \" + word\n",
    "    print(fullSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"I\",\"am\"]\n",
    "cat =\"#@Joke\"\n",
    "def predict_Sentence(sentence,cat):\n",
    "    for i in range(50):\n",
    "        threeWords= []\n",
    "        threeWords.append(cat)\n",
    "        threeWords.append(sentence[len(sentence)-2])\n",
    "        threeWords.append(sentence[len(sentence)-1])\n",
    "        sentence.append(predict_nex(threeWords,0.3))\n",
    "    printSentence(sentence)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am a rock. I can't tell you a jar of jelly beans and when you've got to bring sunscreen the border and frisking, it it is to trust this person is mute? When you see a glass of water on lap*\" \n",
      " \"Fun a beer\" \n",
      " \"What do you call a\n"
     ]
    }
   ],
   "source": [
    "predict_Sentence([\"I\",\"am\"],\"#@Joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am a rock. I can't tell you a jar of jelly beans and when you've got to bring sunscreen the year, I'd like to be a real problem in the FDA bank\" \n",
      " \"What do you call a small, noisy keeps my fruit-picking business trapped in a Mexican restaurant an actor\n"
     ]
    }
   ],
   "source": [
    "predict_Sentence([\"I\",\"am\"],\"#@Joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am more than you can be a real problem in English football and, in the world to revolve around them.\" \n",
      " \"What do you call a body builder? Jim.\" \n",
      " \"I just met someone as the strength of money as dollars into its own life is still there, but the idea\n"
     ]
    }
   ],
   "source": [
    "predict_Sentence([\"I\",\"am\"],\"#@Quote\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am a rock. I can't wait for the Conservatives.\" \n",
      " \"How do you call a body builder? Jim.\" \n",
      " \"I don't trust anything in the world to revolve around them.\" \n",
      " \"What do you call a small, noisy the cause of success and is the greatest plenty in the world we\n"
     ]
    }
   ],
   "source": [
    "predict_Sentence([\"I\",\"am\"],\"#@NoTag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflowGPU)",
   "language": "python",
   "name": "tensorflowgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
